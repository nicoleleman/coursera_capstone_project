{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project - The Battle of Neighbourhoods (Week 1 & 2)\n",
    "<h2><center>Property Prices & Venue Data Analysis of London</center></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Background\n",
    "It goes without saying that the coronavirus (COVID-19) has had, is currently and will continue to have a significant impact on businesses and the economy worldwide. This is evident with stock market and oil prices crash, record breaking number of people filing for unemployment and major airlines on the brink of administration.\n",
    "\n",
    "The Real Estate & Property market is no exception to the coronavirus impact, with the London property market coming to a halt back in March when the full lock down was announced to prevent the spread of the virus. Physical viewings were postponed, constructions were suspended and estate agents & mortgage lenders no longer able to value properties in person.\n",
    "\n",
    "As a result Zoopla has predicted that completed sales in the UK will be 50% lower in 2020 than in 2019 and Knight Frank has also predicted that the number of sales in Greater London will fall by 35%. However despite the bleak outlook for property and housing prices this year, a large number of firms & their analysts believe that the housing market could make a very strong recovery by 2021, with an estimated range of 3% - 6%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Business Problem\n",
    "The best decisions are often backed up by insight and data,  by utilising Machine Learning we can effectively and efficiently generate those insights in order to provide potential home-buyers and investors the best decision making support as possible. This brings us to our business problem: How can we generate insight so home-buyers and investors can make well informed choices when purchasing properties in London, especially in this uncertain economic situation?\n",
    "\n",
    "In order to solve this business problem, we will cluster the London areas based on the average sales price, local venues and amenities, i.e. schools, supermarkets, coffee shops. We will then compare these clusters with the average property prices and rental prices for each borough, and also calculate the rental yield for each cluster for investors who are buying to let. This will provide valuable information on whether a property is a viable choice for home-buyers & investors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Price Paid Data (property sales data) in London will be sourced from HM Land Registry, where the data is based on the raw data released each month. The dataset will include the following columns: Transaction unique identifier, Price, Date of Transfer, Postcode, Property Type, Old/New, Duration, PAON (Primary Addressable Object Name), SAON (Secondary Addressable Object Name), Street, Locality, Town/City, District, County and PPD Category Type.\n",
    "\n",
    "The FourSquare API will be used to access and explore venues and amenities based on the Latitude and Longitude collected using the GeoCoder library, which will then be read into a dataframe for data wrangling and cleaning. This dataframe will be merged with the Price Paid Data from HM Land Registry and processed to be suitable for fitting the machine learning model.\n",
    "\n",
    "The list of boroughs in London will be scrapped from the Wikipedia page and the average property and rental prices per borough will be scraped from Foxtons (A UK estate agency). The data will be visualised using Plotly in order to gauge the recommendations generated by our model against average prices for each cluster.\n",
    "`\n",
    "Please see the References section at the end of the notebook for links and descriptions for data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Data Collecting & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "import requests\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter # count occurrences \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import hdbscan\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price Paid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd_2019 = pd.read_csv('../data/external/pp-2019.csv')\n",
    "ppd_2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned on the 'How to access HM Land Registry Price Paid Data' website, the column headers are not supplied in the file therefore they will need to be manually added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd_2019.columns = ['TUID', 'Price', 'Date_of_Transfer', 'Postcode', 'Property_Type', 'Old_New', 'Duration',\n",
    "                    'PAON', 'SAON', 'Street', 'Locality', 'Town_City', 'District', 'County', 'PPD_Cat_Type', 'Record_Status']\n",
    "\n",
    "ppd_2019.sort_values(by=['Date_of_Transfer'], ascending=False, inplace=True)\n",
    "ppd_2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of London Boroughs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = requests.get('https://en.wikipedia.org/wiki/List_of_London_boroughs').text\n",
    "soup = BeautifulSoup(source)\n",
    "table = soup.find('table',class_='wikitable sortable')\n",
    "tr_elements = soup.find_all(['tr'])[0:34]\n",
    "\n",
    "# Write the table headers and cells into a CSV\n",
    "with open('../data/raw/london_boroughs.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    column_headers = ['Borough','Inner','Status', 'Local authority', 'Political control',\n",
    "                      'Headquarters', 'Area (sq_mi)', 'Population (2013_est)', 'Coordinates', 'Nr in map']\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(column_headers)\n",
    "    for cell in tr_elements:\n",
    "            td = cell.find_all('td')\n",
    "            row = [i.text.replace('\\n','').replace(' / ',',') for i in td]\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were 3 boroughs that were scraped with citation reference text, '[note #]', so those were removed by chaining .replace methods. The latitude and longitdue were also sliced out of the Coordinates column and assigned each to their own respected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_boroughs = pd.read_csv('../data/raw/london_boroughs.csv', usecols=['Borough', 'Coordinates'])\n",
    "london_boroughs['Latitude'] = london_boroughs['Coordinates'].str[43:50]\n",
    "london_boroughs['Longitude'] = london_boroughs['Coordinates'].str[52:60]\n",
    "london_boroughs['Borough'] = [b.replace('[note 1]', '').replace('[note 4]', '').replace('[note 2]', '') for b in london_boroughs['Borough'] ]\n",
    "london_boroughs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Property and Rental Prices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I scraped a list of London postcodes and their corresponding districts from the following website: https://www.doogal.co.uk/london_postcodes.php. Using the postcode I then scraped the average property prices and rental prices from the Foxton website. All the data is written into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/raw/london_property_prices.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    column_headers = ['postcode_prefix','districts', 'avg_property_price','avg_rental_price']\n",
    "    writer = csv.DictWriter(f, fieldnames = column_headers)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Scrape postcodes and districts \n",
    "    source_postcode = requests.get('https://www.doogal.co.uk/london_postcodes.php').text\n",
    "    soup1 = BeautifulSoup(source_postcode)\n",
    "    districts = soup1.find('div', class_='realContent')\n",
    "    a_elements = districts.find_all('a')[2:157]\n",
    "    for i in range(len(a_elements)):\n",
    "        Postcode = a_elements[i].getText().split(':')[0]\n",
    "        try:\n",
    "            Districts= a_elements[i].getText().split(': ')[1]\n",
    "        except:\n",
    "            Districts = 'NaN'\n",
    "        i += 1\n",
    "        \n",
    "        # Scrape the prices for each postcode obtained above\n",
    "        source_foxtons = requests.get('https://www.foxtons.co.uk/living-in/{}'.format(Postcode)).text\n",
    "        soup2 = BeautifulSoup(source_foxtons)\n",
    "        var_elements = soup2.find_all(['var'], class_=\"price_headline\")\n",
    "        \n",
    "        \n",
    "        property_price = var_elements[0].getText()[1:]\n",
    "        try:\n",
    "            rental_price = var_elements[1].getText()[1:]\n",
    "        except:\n",
    "            rental_price = 'NaN'\n",
    "        # Return NaN if there is no data for rental prices\n",
    "        if len(rental_price) > 1:\n",
    "            result = re.search('[0-9A-Fa-f,]+', rental_price).group()\n",
    "        else:\n",
    "            result = 'NaN'\n",
    "            \n",
    "        # Write all of the above into the CSV    \n",
    "        writer.writerow({'postcode_prefix': Postcode, 'districts':Districts,\n",
    "                         'avg_property_price':property_price, 'avg_rental_price':result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_property_prices = pd.read_csv('../data/raw/london_property_prices.csv')\n",
    "london_property_prices.dropna(how='any', inplace=True)\n",
    "london_property_prices['avg_property_price'] = london_property_prices['avg_property_price'].apply(lambda X: X.replace(\",\", \"\"))\n",
    "london_property_prices['avg_rental_price'] = london_property_prices['avg_rental_price'].apply(lambda X: X.replace(\",\", \"\"))\n",
    "london_property_prices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Price Paid Data, most of the columns were dropped as they were not relevant in our business problem, such as TUID, Duration, PAON, SAON, Locality, PPD_Cat_Type and Record_Status. \n",
    "\n",
    "There were also a number of rows where the prices where very high, which could have been a commerical property. Therefore rows where the price is larger than £2,000,000 were also dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features that are irrelevant for this project, filter for London rows and clean up the data\n",
    "ppd_2019_clean = ppd_2019.drop(columns=['TUID', 'Duration', 'PAON', 'SAON', 'Locality', 'PPD_Cat_Type', 'Record_Status'])\n",
    "\n",
    "# Filter out rows where Town_City column contains 'LONDON'\n",
    "ppd_london = ppd_2019_clean[ppd_2019['Town_City']=='LONDON'].copy()\n",
    "ppd_london = ppd_london.drop(ppd_london[ppd_london.Price > 2000000].index)\n",
    "ppd_london.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "# Add a new column for the postcode prefixes\n",
    "ppd_london['Postcode_Prefix'] = ppd_london['Postcode'].apply(lambda x: x.split(' ')[0])\n",
    "ppd_london.sort_values('Street')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd_grouped = ppd_london.groupby(['Street', 'District', 'Postcode_Prefix'])['Price'].mean().round(0).reset_index()\n",
    "ppd_grouped.columns = ['street', 'district', 'postcode_prefix', 'avg_price']\n",
    "ppd_grouped.sort_values(by=['street'], inplace=True)\n",
    "ppd_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we filter out rows from the ppd_2019_clean dataframe where the Town is 'LONDON', then we group the dataframe by the street names and find the average price paid for property on those streets.\n",
    "\n",
    "As there are a large number of rows, getting the latitude, longitude and FourSquare data for each row/street will take a significant amount of time. A Python script will be used to get all the latitude and longitude, write them to a CSV file.\n",
    "\n",
    "This will decrease the computional time required and provide us with an overview of properties and their nearby venues across different pricing ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd_london_2019 = pd.read_csv('../data/processed/ppd_london_2019.csv')\n",
    "ppd_london_2019.dropna(axis=0, how='any', inplace=True)\n",
    "ppd_london_2019.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd_london_2019['latitude'] = pd.to_numeric(ppd_london_2019['latitude'], downcast=\"float\")\n",
    "ppd_london_2019['longitude'] = pd.to_numeric(ppd_london_2019['longitude'], downcast=\"float\")\n",
    "ppd_london_2019.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are over 14000 rows in the dataframe above, if we were to get venue data using the FourSquare API the compuntational time required will be significant. In addition, an application can only make a maximum of 5000 requests per hour to the venues endpoint. In order to reduce the dataset without causing any data bias, we will use the .sample method and sample 20% of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a sample from this dataframe\n",
    "ppd_london_2019_sample = ppd_london_2019.sample(frac=0.2, replace=False, random_state=1).copy()\n",
    "ppd_london_2019_sample = ppd_london_2019_sample.sort_values('street').reset_index(drop=True)\n",
    "ppd_london_2019_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = len(ppd_london_2019_sample[ppd_london_2019_sample['avg_price'] < 1000000]) \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ppd_london_2019_sample[ppd_london_2019_sample['avg_price'] < 1000000]).describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ppd_london_2019_sample[ppd_london_2019_sample['avg_price'] > 1000000]).describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Price Paid Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the average property prices for those streets by plotting them on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent='london_explorer')\n",
    "location = geolocator.geocode('London, UK')\n",
    "latitude_ldn = location.latitude\n",
    "longitude_ldn = location.longitude\n",
    "print('The geographical coordinate of London, UK are {}, {}.'.format(latitude_ldn, longitude_ldn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapbox_access_token = (open(\"../secrets/mapbox_token.txt\").read())\n",
    "\n",
    "fig = px.scatter_mapbox(ppd_london_2019_sample, lat=\"latitude\", lon=\"longitude\", size=\"avg_price\", color=\"avg_price\",\n",
    "                  color_continuous_scale=px.colors.cyclical.IceFire, size_max=10, width=1000, height=700)\n",
    "fig.update_layout(\n",
    "    title='Property Paid Price in London 2019 (Capped at £2 million)',\n",
    "    autosize=True,\n",
    "    hovermode='closest',\n",
    "    showlegend=True,\n",
    "    mapbox=dict(\n",
    "        accesstoken=mapbox_access_token,\n",
    "        bearing=0,\n",
    "        center=dict(\n",
    "            lat=latitude_ldn,\n",
    "            lon=longitude_ldn\n",
    "        ),\n",
    "        pitch=5,\n",
    "        zoom=10,\n",
    "        style='light'\n",
    "    ),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, neighbourhoods such as Mayfair, Chelsea, Knightsbridge, Notting Hill and Fulham have the highest average property prices. We can also see from the map above that more expensive properties are mostly located on the west side of central London, and compared to the east side there are far fewer properties that exceed the £1,000,000 mark. However there are exceptions, with a small cluster near Blackheath, Canary Wharf, Newbury Park and Bexleyheath.\n",
    "\n",
    "This will be useful to home-buyers or investors as they may take into consideration a neighbourhood that they were not aware of previously. The next step would be to explore the said neighbourhoods using the FourSquare API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Explore the area and nearby venues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first neighbourhood and it's nearby venues within a 300 meter radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_dict = {}\n",
    "with open('../secrets/foursquare_secrets.txt') as f:\n",
    "    for item in f:\n",
    "        (key, val) = item.split(':')\n",
    "        secret_dict[key] = val.strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 100\n",
    "radius = 300\n",
    "VERSION = '20180605'\n",
    "neighborhood_latitude = ppd_london_2019_sample.loc[0, 'latitude']\n",
    "neighborhood_longitude = ppd_london_2019_sample.loc[0, 'longitude']\n",
    "neighborhood_name = ppd_london_2019_sample.loc[0, 'street']\n",
    "url = 'https://api.foursquare.com/v2/venues/explore?client_id={}&client_secret={}&ll={},{}&v={}&radius={}&limit={}'.format(secret_dict.get('client_id'), secret_dict.get('client_secret'), neighborhood_latitude, neighborhood_longitude, VERSION, radius, LIMIT)\n",
    "results = requests.get(url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that extracts the category of the venue\n",
    "def get_category_type(row):\n",
    "    try:\n",
    "        categories_list = row['categories']\n",
    "    except:\n",
    "        categories_list = row['venue.categories']\n",
    "        \n",
    "    if len(categories_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return categories_list[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues = results['response']['groups'][0]['items']\n",
    "    \n",
    "nearby_venues = pd.json_normalize(venues) # flatten JSON\n",
    "\n",
    "# filter columns\n",
    "filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']\n",
    "nearby_venues =nearby_venues.loc[:, filtered_columns]\n",
    "\n",
    "# filter the category for each row\n",
    "nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)\n",
    "\n",
    "# clean columns\n",
    "nearby_venues.columns = [col.split(\".\")[-1] for col in nearby_venues.columns]\n",
    "\n",
    "nearby_venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} venues were returned by Foursquare.'.format(nearby_venues.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we repeat what we have done above for all the other neighbourhoods by creating a function that repeat the same process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below takes in 4 variables and 1 default variable, it then loops over each row in the datafraem and sends the API call to FourSquare. The JSON data returned is then processed to extract the data that we are after, in this case they are Venue name, Venue latitude, Venue longitude and Venue category. Finally the data is written into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNearbyVenues(names, districts, latitudes, longitudes, radius=300):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, dstr, lat, lng in zip(names, districts, latitudes, longitudes):\n",
    "        print(name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            secret_dict.get('client_id'), \n",
    "            secret_dict.get('client_secret'), \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        try:\n",
    "            results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        except:\n",
    "            continue\n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name,\n",
    "            dstr,\n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Street', \n",
    "                             'Street District', \n",
    "                             'Street Latitude', \n",
    "                             'Street Longitude', \n",
    "                             'Venue', \n",
    "                             'Venue Latitude',\n",
    "                             'Venue Longitude',\n",
    "                             'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_venues = getNearbyVenues(names=ppd_london_2019_sample['street'],\n",
    "                                  districts=ppd_london_2019_sample['district'],\n",
    "                            latitudes=ppd_london_2019_sample['latitude'],\n",
    "                            longitudes=ppd_london_2019_sample['longitude']\n",
    "                            )\n",
    "district_venues.groupby(['Street', 'Street District'])['Venue'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FourSquare venue dataframe is pickled using the pandas .to_pickle method. This will eliminate the need to re-run the FourSquare venue calls above, thus saving time between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_venues.to_pickle('../data/processed/london_venues.pkl')  # saving the dataframe as a .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_venues= pd.read_pickle('../data/processed/london_venues.pkl')\n",
    "print(london_venues.shape)\n",
    "london_venues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} unique categories.'.format(len(london_venues['Venue Category'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the top 25 venues from the FourSquare data we collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_venues_top25 = london_venues.groupby(['Venue Category'])['Venue Category'].count()\\\n",
    "    .reset_index(name=\"count\").sort_values(['count'], ascending=False)[0:25]\n",
    "\n",
    "fig2 = px.bar(london_venues_top25, x='Venue Category', y='count', labels={'x':'Venue Categories', 'y':'Count'}, text='count')\n",
    "fig2.update_layout(title='Top 25 Venue Categories',)\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Analyse each neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.max_columns = 15\n",
    "\n",
    "london_onehot = pd.get_dummies(london_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "# There was a 'Neighborhood' venue category which needed to be dropped as it was skewing the results\n",
    "#ondon_onehot.drop('Neighborhood', axis = 1, inplace=True)\n",
    "\n",
    "london_onehot_wstreets = pd.get_dummies(london_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "# add neighborhood column back to dataframe as the first column\n",
    "london_onehot_wstreets.insert(loc=0, column='street', value=london_venues['Street'])\n",
    "london_onehot_wstreets.insert(loc=1, column='district', value=london_venues['Street District'])\n",
    "\n",
    "print(london_onehot_wstreets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_onehot_grouped = london_onehot_wstreets.groupby(['street', 'district']).mean().reset_index()\n",
    "\n",
    "print(london_onehot_grouped.shape)\n",
    "london_onehot_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we write a function to get the top 10 venues for each neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[2:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 10\n",
    "\n",
    "# for assigning indicators to 1st, 2nd & 3rd\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['street', 'district']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "neighborhoods_venues_sorted = pd.DataFrame(columns=columns)\n",
    "neighborhoods_venues_sorted['street'] = london_onehot_grouped['street']\n",
    "neighborhoods_venues_sorted['district'] = london_onehot_grouped['district']\n",
    "\n",
    "for ind in np.arange(london_onehot_grouped.shape[0]):\n",
    "    neighborhoods_venues_sorted.iloc[ind, 2:] = return_most_common_venues(london_onehot_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "print(neighborhoods_venues_sorted.shape)\n",
    "neighborhoods_venues_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1. Optimising K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Elbow Method is used to determine the optimal value of k as this is one of the most popular methods. We will be using 2 metric values calculated from a range of k values in order to determine the 'elbow point', i.e. the point after which the metrics starts decreasing linearly.\n",
    "\n",
    "Those 2 metric values are:\n",
    "- Distortion: Calculated as the average of the squared distances from the cluster centres of the respective clusters where typically the Euclidean distance is used.\n",
    "- Inertia: The sum of squared distances of samples to their closest cluster centre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "distortions = []\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(ppd_grouped_clustering)\n",
    "    distortions.append(sum(np.min(cdist(ppd_grouped_clustering, km.cluster_centers_, \n",
    "                      'euclidean'),axis=1)) / ppd_grouped_clustering.shape[0]) \n",
    "    Sum_of_squared_distances.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.plot(K, distortions, 'bx-')\n",
    "ax2.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "ax1.set_title('The Elbow Method using Distortion', fontsize = 15)\n",
    "ax2.set_title('The Elbow Method using Inertia', fontsize = 15)\n",
    "ax1.set_ylabel('Distortion', fontsize = 12)\n",
    "ax2.set_ylabel('Sum_of_squared_distances', fontsize = 12)\n",
    "ax1.set_xlabel('k', fontsize = 12)\n",
    "ax2.set_xlabel('k', fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of clusters\n",
    "kclusters = 5\n",
    "\n",
    "ppd_grouped_clustering = london_onehot_grouped.drop(labels=['street', 'district'], axis=1)\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0, n_init=50).fit(ppd_grouped_clustering)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_\n",
    "# neighborhoods_venues_sorted = []\n",
    "print(Counter(kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add clustering labels\n",
    "neighborhoods_venues_clustered = neighborhoods_venues_sorted.copy()\n",
    "neighborhoods_venues_clustered.insert(loc = 2, column = 'Cluster Labels', value = kmeans.labels_)\n",
    "\n",
    "ppd_london_merged = ppd_london_2019_sample.copy()\n",
    "# merge manhattan_grouped with manhattan_data to add latitude/longitude for each neighborhood\n",
    "ppd_london_merged = ppd_london_merged.join(neighborhoods_venues_clustered.set_index(['street', 'district']), on=['street', 'district'], how='left')\n",
    "\n",
    "ppd_london_merged\n",
    "# check the last columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd_london_merged.sort_values(by = '1st Most Common Venue', ascending = False)\n",
    "ppd_london_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd_london_merged.dropna(axis=0, how='any', inplace=True)\n",
    "ppd_london_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd_london_merged['Cluster Labels'] = ppd_london_merged['Cluster Labels'].astype(int)\n",
    "ppd_london_merged = ppd_london_merged.sort_values('Cluster Labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we visualise the clusters using an interactive Plotly map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = px.scatter_mapbox(ppd_london_merged, lat=\"latitude\", lon=\"longitude\",\n",
    "                        color=ppd_london_merged[\"Cluster Labels\"].astype(str), hover_data=['street'], width=1000, height=700)\n",
    "\n",
    "fig3.update_layout(\n",
    "    title='Clustering London Neighbourhoods (K-Means)',\n",
    "    autosize=True,\n",
    "    hovermode='closest',\n",
    "    showlegend=True,\n",
    "    mapbox=dict(\n",
    "        accesstoken=mapbox_access_token,\n",
    "        bearing=0,\n",
    "        center=dict(lat=latitude_ldn, lon=longitude_ldn), \n",
    "        pitch=5,\n",
    "        zoom=10,\n",
    "        style='light'\n",
    "    ),\n",
    "    legend={'title':'Clusters', 'traceorder':'normal'}\n",
    ")\n",
    "fig3.update_traces(marker=dict(size=10))\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 0 - Pubs & Coffee Shops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0 = ppd_london_merged.loc[ppd_london_merged['Cluster Labels'] == 0, \n",
    "                      ppd_london_merged.columns[[0,1] + list(range(4, ppd_london_merged.shape[1]))]]\n",
    "cluster_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 1 - Parks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1 = ppd_london_merged.loc[ppd_london_merged['Cluster Labels'] == 1, \n",
    "                      ppd_london_merged.columns[[0,1] + list(range(5, ppd_london_merged.shape[1]))]]\n",
    "cluster_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 2 - Pubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_2 = ppd_london_merged.loc[ppd_london_merged['Cluster Labels'] == 2, \n",
    "                      ppd_london_merged.columns[[0,1] + list(range(5, ppd_london_merged.shape[1]))]]\n",
    "cluster_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 3 - Cafes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_3 = ppd_london_merged.loc[ppd_london_merged['Cluster Labels'] == 3, \n",
    "                      ppd_london_merged.columns[[0,1] + list(range(5, ppd_london_merged.shape[1]))]]\n",
    "cluster_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 4 - Grocery Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_4 = ppd_london_merged.loc[ppd_london_merged['Cluster Labels'] == 4, \n",
    "                      ppd_london_merged.columns[[0,1] + list(range(5, ppd_london_merged.shape[1]))]]\n",
    "cluster_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the top 5 venues in each cluster onto a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd_london_merged_top5 = ppd_london_merged.groupby([\"Cluster Labels\", \"1st Most Common Venue\"])['Cluster Labels'].count()\\\n",
    ".reset_index(name=\"count\").sort_values(['Cluster Labels','count'], ascending=False)\n",
    "\n",
    "cluster_0_top5 = (ppd_london_merged_top5[ppd_london_merged_top5['Cluster Labels'] == 0][0:5])\n",
    "cluster_1_top5 = (ppd_london_merged_top5[ppd_london_merged_top5['Cluster Labels'] == 1][0:5])\n",
    "cluster_2_top5 = (ppd_london_merged_top5[ppd_london_merged_top5['Cluster Labels'] == 2][0:5])\n",
    "cluster_3_top5 = (ppd_london_merged_top5[ppd_london_merged_top5['Cluster Labels'] == 3][0:5])\n",
    "cluster_4_top5 = (ppd_london_merged_top5[ppd_london_merged_top5['Cluster Labels'] == 4][0:5])\n",
    "cluster_5_top5 = (ppd_london_merged_top5[ppd_london_merged_top5['Cluster Labels'] == 5][0:5])\n",
    "cluster_6_top5 = (ppd_london_merged_top5[ppd_london_merged_top5['Cluster Labels'] == 6][0:5])\n",
    "cluster_7_top5 = (ppd_london_merged_top5[ppd_london_merged_top5['Cluster Labels'] == 7][0:5])\n",
    "\n",
    "\n",
    "top_5_venues = cluster_0_top5.append([cluster_1_top5, cluster_2_top5, cluster_3_top5, \n",
    "                                      cluster_4_top5, cluster_5_top5, cluster_6_top5,\n",
    "                                     cluster_7_top5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig4 = px.bar(top_5_venues, x=\"Cluster Labels\", y=\"count\", color='1st Most Common Venue',\n",
    "             height=500)\n",
    "\n",
    "fig4.update_layout(title='Clustering London Streets', \n",
    "                   barmode='stack',\n",
    "                   bargap=0.15,\n",
    "                   bargroupgap=0.1, \n",
    "                   legend={'title':'Venue Category',\n",
    "                          'traceorder':'normal'}\n",
    "                  )\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_0_top5)\n",
    "print(cluster_1_top5)\n",
    "print(cluster_2_top5)\n",
    "print(cluster_3_top5)\n",
    "print(cluster_4_top5)\n",
    "ppd_london_merged_top5[ppd_london_merged_top5['Cluster Labels'] == 0]['count'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, cluster 0, 1 and 2 all have a high number of pubs which is not surprising as these clusters are located around Central London where there are more then 3500 pubs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. K - Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmode_onehot_grouped = london_onehot_wstreets.groupby(['street', 'district']).sum().reset_index()\n",
    "kmode_grouped_clustering = kmode_onehot_grouped.drop(labels=['street', 'district'], axis=1)\n",
    "set(kmode_grouped_clustering['Pub'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmodes.kmodes import KModes\n",
    "\n",
    "# define the k-modes model\n",
    "km = KModes(n_clusters=5, init='Huang', n_init=10, verbose=1)\n",
    "\n",
    "# fit the clusters to the skills dataframe\n",
    "clusters = km.fit_predict(kmode_grouped_clustering)\n",
    "\n",
    "# get an array of cluster modes\n",
    "kmodes = km.cluster_centroids_\n",
    "shape = kmodes.shape\n",
    "\n",
    "# For each cluster mode (a vector of \"1\" and \"0\")\n",
    "# find and print the column headings where \"1\" appears.\n",
    "# If no \"1\" appears, assign to \"no-skills\" cluster.\n",
    "for i in range(shape[0]):\n",
    "    if sum(kmodes[i,:]) == 0:\n",
    "        print(\"\\ncluster \" + str(i) + \": \")\n",
    "        print(\"No venues cluster\")\n",
    "    else:\n",
    "        print(\"\\ncluster \" + str(i) + \": \")\n",
    "        cent = kmodes[i,:]\n",
    "        for j in kmode_grouped_clustering.columns[np.nonzero(cent)]:\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kmodedf = neighborhoods_venues_sorted.copy()\n",
    "kmodedf.insert(loc = 2, column = 'Cluster Labels', value = clusters)\n",
    "kmodedf.sort_values('Cluster Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmode_merge = ppd_london_2019_sample\n",
    "\n",
    "# merge manhattan_grouped with manhattan_data to add latitude/longitude for each neighborhood\n",
    "kmode_merge = kmode_merge.join(kmodedf.set_index(['street', 'district']), on=['street', 'district'])\n",
    "kmode_merge.dropna(inplace=True)\n",
    "kmode_merge.sort_values('Cluster Labels')\n",
    "\n",
    "fig10 = px.scatter_mapbox(kmode_merge, lat=\"latitude\", lon=\"longitude\",\n",
    "                        color=kmode_merge[\"Cluster Labels\"].astype(str), hover_data=['street'], width=1000, height=700)\n",
    "\n",
    "fig10.update_layout(\n",
    "    title='Clustering London Neighbourhoods (K-Mode, k = 4)',\n",
    "    autosize=True,\n",
    "    hovermode='closest',\n",
    "    showlegend=True,\n",
    "    mapbox=dict(\n",
    "        accesstoken=mapbox_access_token,\n",
    "        bearing=0,\n",
    "        center=dict(lat=latitude_ldn, lon=longitude_ldn), \n",
    "        pitch=5,\n",
    "        zoom=10,\n",
    "        style='light'\n",
    "    ),\n",
    "    legend={'title':'Clusters', 'traceorder':'normal'}\n",
    ")\n",
    "fig10.update_traces(marker=dict(size=10))\n",
    "fig10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmodedf_top5 = kmode_merge.groupby([\"Cluster Labels\", \"1st Most Common Venue\"])['Cluster Labels'].count()\\\n",
    ".reset_index(name=\"count\").sort_values(['Cluster Labels','count'], ascending=False)\n",
    "\n",
    "kcluster_0_top5 = (kmodedf_top5[kmodedf_top5['Cluster Labels'] == 0][0:5])\n",
    "kcluster_1_top5 = (kmodedf_top5[kmodedf_top5['Cluster Labels'] == 1][0:5])\n",
    "kcluster_2_top5 = (kmodedf_top5[kmodedf_top5['Cluster Labels'] == 2][0:5])\n",
    "kcluster_3_top5 = (kmodedf_top5[kmodedf_top5['Cluster Labels'] == 3][0:5])\n",
    "kcluster_4_top5 = (kmodedf_top5[kmodedf_top5['Cluster Labels'] == 4][0:5])\n",
    "\n",
    "kmode_top_5_venues = kcluster_0_top5.append([kcluster_1_top5, kcluster_2_top5, kcluster_3_top5, \n",
    "                                      kcluster_4_top5], ignore_index=True)\n",
    "\n",
    "kmode_top_5_venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig11 = px.bar(kmode_top_5_venues, x=\"Cluster Labels\", y=\"count\", color='1st Most Common Venue', height=500)\n",
    "fig11.update_layout(title='Clustering London Streets', \n",
    "                   barmode='stack',\n",
    "                   bargap=0.15,\n",
    "                   bargroupgap=0.1, \n",
    "                   legend={'title':'Venue Category',\n",
    "                          'traceorder':'normal'}\n",
    "                  )\n",
    "fig11.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimising K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = []\n",
    "for num_clusters in list(range(1,15)):\n",
    "    kmode = KModes(n_clusters=num_clusters, init = \"Huang\", n_init = 5, verbose=1)\n",
    "    kmode.fit_predict(kmode_grouped_clustering)\n",
    "    cost.append(kmode.cost_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(7.5, 5))\n",
    "ax1.plot(y, cost, 'bx-')\n",
    "ax1.set_title('k modes', fontsize = 15)\n",
    "ax1.set_ylabel('Distortion', fontsize = 12)\n",
    "ax1.set_xlabel('k', fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Housing Prices 1995 - 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average PPD per borough from 1995 - 2017\n",
    "historical_london_ppd = pd.read_csv('../data/external/land-registry-house-prices-ward.csv')\n",
    "historical_london_ppd = historical_london_ppd[(historical_london_ppd['Measure'] == 'Mean') & (historical_london_ppd['Value'] != '-')]\n",
    "historical_london_ppd['Year'] = historical_london_ppd['Year'].apply(lambda x : x [-4:])\n",
    "historical_london_ppd['Value'] = historical_london_ppd['Value'].apply(lambda X: X.replace(\",\", \"\")).astype(int)\n",
    "\n",
    "borough_avg_ppd = historical_london_ppd.groupby(['Year', 'Borough'])['Value'].mean().round(2).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the csv and calculate the PPD per borough in 2018 and 2020 (March)\n",
    "ppd_2018 = pd.read_csv('../data/external/pp-2018.csv')\n",
    "\n",
    "ppd_2018.columns = ['TUID', 'Price', 'Date_of_Transfer', 'Postcode', 'Property_Type', 'Old_New', 'Duration',\n",
    "                    'PAON', 'SAON', 'Street', 'Locality', 'Town_City', 'District', 'County', 'PPD_Cat_Type', 'Record_Status']\n",
    "\n",
    "# Drop features that are irrelevant for this project, filter for London rows and clean up the data\n",
    "ppd_2018_clean = ppd_2018.drop(columns=['TUID', 'Duration', 'PAON', 'SAON', 'Locality', 'PPD_Cat_Type', 'Record_Status'])\n",
    "\n",
    "# Filter out rows where Town_City column contains 'LONDON'\n",
    "ppd_london_2018 = ppd_2018_clean[ppd_2018['Town_City']=='LONDON'].copy()\n",
    "ppd_london_2018.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "avg_ppd_borough_2018 = ppd_london_2018.groupby('District')['Price'].mean().round(2).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map animation of mean PPD 1995 - 2017\n",
    "with open('../data/external/london_boroughs_proper.geojson', 'r') as response:\n",
    "    boroughs = json.load(response)\n",
    "\n",
    "fig11 = px.choropleth_mapbox(borough_avg_ppd, geojson=boroughs, locations='Borough', color='Value',\n",
    "                            range_color=(0, 2000000),\n",
    "                            animation_frame='Year',\n",
    "                            color_continuous_scale=\"Viridis\",\n",
    "                            mapbox_style=\"carto-positron\",\n",
    "                            zoom=9, center = {\"lat\": latitude_ldn, \"lon\": longitude_ldn},\n",
    "                            featureidkey='properties.name',\n",
    "                            opacity=0.5,\n",
    "                           labels={'Value':'Avg Property Prices'}\n",
    "                          )\n",
    "fig11.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig11.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Housing Prices 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average PPD per borough in 2019\n",
    "avg_ppd_borough_2019 = ppd_london.groupby('District')['Price'].mean().round(2).reset_index()# Removing rows where district = Epping Forest\n",
    "avg_ppd_borough_2019 = avg_ppd_borough_2019[avg_ppd_borough_2019.District != 'EPPING FOREST'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig12 = px.bar(avg_ppd_borough_2019, x='District', y='Price', color='Price', range_y=[0,1000000],\n",
    "              color_continuous_scale=[\"navy\", \"springgreen\", \"gold\"])\n",
    "fig12.layout.update(title={'text':'Average Property Prices in London for 2019'},\n",
    "                   xaxis={'title':{'text':'Borough',\n",
    "                                  'font':{'size':17}},\n",
    "                          'tickangle':30,\n",
    "                          'tickfont':{'size':12}\n",
    "                         },\n",
    "                   yaxis={'title':{'text':'Value in GBP (£)',\n",
    "                                  'font':{'size':17}},\n",
    "                         },\n",
    "                  )\n",
    "fig12.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig12a = px.bar(testdf, x='District', y='Price', text='Price')\n",
    "fig12a.update_traces(texttemplate='%{text:.2s}', textposition='outside')\n",
    "fig12a.layout.update(title={'text':'Number of property sales in 2019'},\n",
    "                   xaxis={'title':{'text':'Borough',\n",
    "                                  'font':{'size':17}},\n",
    "                          'tickangle':30,\n",
    "                          'tickfont':{'size':12}\n",
    "                         },\n",
    "                   yaxis={'title':{'text':'Count',\n",
    "                                  'font':{'size':17}},\n",
    "                         },\n",
    "                  )\n",
    "fig12a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ppd_borough_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig13 = px.bar(borough_avg_ppd, x='Borough', y='Value', color='Value', animation_frame='Year',\n",
    "           hover_name='Borough', range_y=[0,2000000])\n",
    "\n",
    "fig13.layout.update(title={'text':'Average Property Prices in London (1995-2017)'},\n",
    "                   xaxis={'title':{'text':'Borough',\n",
    "                                  'font':{'size':17}},\n",
    "                          'tickangle':30,\n",
    "                          'tickfont':{'size':12}\n",
    "                         },\n",
    "                   yaxis={'title':{'text':'Value in GBP (£)',\n",
    "                                  'font':{'size':17}},\n",
    "                         },\n",
    "                  sliders=[{'visible':True,\n",
    "                           'currentvalue':{'prefix':'Year: ',\n",
    "                                           'font':{'size':20},\n",
    "                                           'xanchor':'right',\n",
    "                                           'visible':True},\n",
    "                            'pad':{'t':100},\n",
    "                            'transition':{'duration':20,\n",
    "                                         'easing':'linear'}\n",
    "                            \n",
    "                           }\n",
    "                          ],\n",
    "                   updatemenus=[{'pad':{'t':135}}\n",
    "                               ]\n",
    "                  )\n",
    "fig13.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Comparing PPD with Foxton data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ppd_postcode_2019 = ppd_london_2019_sample.copy()\n",
    "avg_ppd_postcode_2019_df = avg_ppd_postcode_2019.groupby('postcode_prefix')['avg_price'].mean().reset_index(name='avg_property_price_2019').round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd_london_compare = ppd_london_merged.copy()\n",
    "ppd_london_compare = pd.merge(ppd_london_compare, london_property_prices[['postcode_prefix', 'avg_rental_price']], on='postcode_prefix')\n",
    "ppd_london_compare = pd.merge(ppd_london_compare, avg_ppd_postcode_2019_df[['postcode_prefix', 'avg_property_price_2019']], on='postcode_prefix')\n",
    "ppd_london_compare\n",
    "\n",
    "ppd_london_compare['avg_price'] = ppd_london_compare['avg_price'].astype(int)\n",
    "ppd_london_compare['avg_rental_price'] = ppd_london_compare['avg_rental_price'].astype(int)\n",
    "ppd_london_compare['avg_property_price_2019'] = ppd_london_compare['avg_property_price_2019'].astype(int)\n",
    "ppd_london_compare.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd_london_compare['below_average'] = ppd_london_compare.apply(lambda x: x['avg_price'] < x['avg_property_price_2019'], axis=1)\n",
    "ppd_london_compare['rental_yield'] = ppd_london_compare.apply(lambda x: ((x['avg_rental_price']*52)/x['avg_price'])*100, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd_london_compare = ppd_london_compare[ppd_london_compare['avg_price'] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig13 = px.scatter_mapbox(ppd_london_compare, lat=\"latitude\", lon=\"longitude\",\n",
    "                        color=ppd_london_compare['below_average'], hover_data=['street'], width=1000, height=700)\n",
    "\n",
    "fig13.update_layout(\n",
    "    title='Above or below average property prices',\n",
    "    autosize=True,\n",
    "    hovermode='closest',\n",
    "    showlegend=True,\n",
    "    mapbox=dict(\n",
    "        accesstoken=mapbox_access_token,\n",
    "        bearing=0,\n",
    "        center=dict(lat=latitude_ldn, lon=longitude_ldn), \n",
    "        pitch=5,\n",
    "        zoom=10,\n",
    "        style='light'\n",
    "    ),\n",
    "    legend={'title':'Clusters', 'traceorder':'normal'}\n",
    ")\n",
    "fig13.update_traces(marker=dict(size=10))\n",
    "fig13.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig14df = ppd_london_compare.groupby(['Cluster Labels','below_average'])['below_average'].count().reset_index(name='count')\n",
    "fig14 = px.bar(fig14df, x='Cluster Labels', y='count', color='below_average', barmode='group', text='count')\n",
    "fig14.update_traces(textposition='outside')\n",
    "fig14.layout.update(title={'text':'Number of properties that are above & below the 2020 average property prices'},\n",
    "                   xaxis={'title':{'text':'Cluster Labels',\n",
    "                                  'font':{'size':15}},\n",
    "                          'tickfont':{'size':12}\n",
    "                         },\n",
    "                   yaxis={'title':{'text':'Count',\n",
    "                                  'font':{'size':15}},\n",
    "                         },\n",
    "                    legend_title_text='Below average'\n",
    "                  )\n",
    "fig14.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig15df = ppd_london_compare.groupby(['district','below_average'])['below_average'].count().reset_index(name='count')\n",
    "fig15 = px.bar(fig15df, x='district', y='count', color='below_average', barmode='group')\n",
    "fig15.layout.update(title={'text':'Number of properties that are above & below the 2020 average property prices'},\n",
    "                   xaxis={'title':{'text':'Borough',\n",
    "                                  'font':{'size':15}},\n",
    "                          'tickfont':{'size':12},\n",
    "                          'tickangle':30,\n",
    "                         },\n",
    "                   yaxis={'title':{'text':'Count',\n",
    "                                  'font':{'size':15}},\n",
    "                         },\n",
    "                    legend_title_text='Below average'\n",
    "                  )\n",
    "fig15.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig16df = ppd_london_compare.groupby(['Cluster Labels'])['rental_yield'].mean().reset_index()\n",
    "\n",
    "fig16 = px.bar(fig16df, x='Cluster Labels', y='rental_yield', color='rental_yield', barmode='group', text='rental_yield',\n",
    "              range_y=[0,30], color_continuous_scale=[\"navy\", \"springgreen\", \"gold\"])\n",
    "fig16.update_traces(texttemplate='%{text:.2s}', textposition='outside')\n",
    "fig16.layout.update(title={'text':'Average % rental yield in each cluster'},\n",
    "                   xaxis={'title':{'text':'Cluster Labels',\n",
    "                                  'font':{'size':15}},\n",
    "                          'tickfont':{'size':12}\n",
    "                         },\n",
    "                   yaxis={'title':{'text':'Count',\n",
    "                                  'font':{'size':15}},\n",
    "                         },\n",
    "                    legend_title_text='Below'\n",
    "                  )\n",
    "fig16.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig17df = ppd_london_compare.groupby(['district'])['rental_yield'].mean().reset_index()\n",
    "fig17 = px.bar(fig17df, x='district', y='rental_yield', color='rental_yield', barmode='group',\n",
    "               range_y=[0,50], color_continuous_scale=[\"navy\", \"springgreen\", \"gold\"], text='rental_yield')\n",
    "\n",
    "fig17.update_traces(texttemplate='%{text:.2s}', textposition='outside')\n",
    "fig17.layout.update(title={'text':'Average % rental yield in each borough'},\n",
    "                   xaxis={'title':{'text':'Borough',\n",
    "                                  'font':{'size':15}},\n",
    "                          'tickfont':{'size':12},\n",
    "                          'tickangle':30,\n",
    "                         },\n",
    "                   yaxis={'title':{'text':'Percentage %',\n",
    "                                  'font':{'size':15}},\n",
    "                         },\n",
    "                    legend_title_text='Below average'\n",
    "                  )\n",
    "fig17.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Pubs vs No Pubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd_london_merged.dropna(inplace=True)\n",
    "with_pub = ppd_london_merged[(ppd_london_merged['1st Most Common Venue'] == 'Grocery Store') | \\\n",
    "                             (ppd_london_merged['2nd Most Common Venue'] == 'Grocery Store') | \\\n",
    "                             (ppd_london_merged['3rd Most Common Venue'] == 'Grocery Store') | \\\n",
    "                             (ppd_london_merged['4th Most Common Venue'] == 'Grocery Store') | \\\n",
    "                             (ppd_london_merged['5th Most Common Venue'] == 'Grocery Store') | \\\n",
    "                             (ppd_london_merged['6th Most Common Venue'] == 'Grocery Store') | \\\n",
    "                             (ppd_london_merged['7th Most Common Venue'] == 'Grocery Store') | \\\n",
    "                             (ppd_london_merged['8th Most Common Venue'] == 'Grocery Store') | \\\n",
    "                             (ppd_london_merged['9th Most Common Venue'] == 'Grocery Store') | \\\n",
    "                             (ppd_london_merged['10th Most Common Venue'] == 'Grocery Store')\n",
    "                            ].copy()\n",
    "with_pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_pub = ppd_london_merged[(ppd_london_merged['1st Most Common Venue'] != 'Grocery Store') & \\\n",
    "                                (ppd_london_merged['2nd Most Common Venue'] != 'Grocery Store') & \\\n",
    "                                (ppd_london_merged['3rd Most Common Venue'] != 'Grocery Store') & \\\n",
    "                                (ppd_london_merged['4th Most Common Venue'] != 'Grocery Store') & \\\n",
    "                                (ppd_london_merged['5th Most Common Venue'] != 'Grocery Store') & \\\n",
    "                                (ppd_london_merged['6th Most Common Venue'] != 'Grocery Store') & \\\n",
    "                                (ppd_london_merged['7th Most Common Venue'] != 'Grocery Store') & \\\n",
    "                                (ppd_london_merged['8th Most Common Venue'] != 'Grocery Store') & \\\n",
    "                                (ppd_london_merged['9th Most Common Venue'] != 'Grocery Store') & \\\n",
    "                                (ppd_london_merged['10th Most Common Venue'] != 'Grocery Store')\n",
    "                               ].copy()\n",
    "without_pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with_pub['Key'] = 'with_pub'\n",
    "without_pub['Key'] = 'without_pub'\n",
    "\n",
    "df_pub = pd.concat([with_pub, without_pub], keys=['with_pub', 'without_pub'])\n",
    "df_pub_grouped = df_pub.groupby('Key')['avg_price'].mean().reset_index()\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "fig = sns.barplot(data=df_pub_grouped, y='avg_price', x='Key', ax=ax)\n",
    "fig.set_title('Average property prices within 300 meters of a grocery store vs without', fontsize=15)\n",
    "fig.set_ylabel('Value in GBP (£)', fontsize=15)\n",
    "fig.set_xlabel('')\n",
    "fig.set_xticklabels(labels=['With grocery store', 'Without grocery store'], fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Most common venue in each borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by district and 1st Most Common Venue, then count the number of venues per district.\n",
    "borough_top_venue = neighborhoods_venues_sorted.groupby(['district','1st Most Common Venue'])['1st Most Common Venue']\\\n",
    "    .count().reset_index(name='count').copy()\n",
    "borough_top_venue.sort_values(by=['district','count'], ascending=False)\n",
    "borought_top_venue_unique = borough_top_venue.loc[borough_top_venue.reset_index().groupby(['district'])['count'].idxmax()]\n",
    "borought_top_venue_unique = borought_top_venue_unique[borought_top_venue_unique.district != 'EPPING FOREST'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(17, 5))\n",
    "fig = sns.barplot(data=borought_top_venue_unique, x='district', y='count', ax=ax, ci=None, hue='1st Most Common Venue', dodge=False)\n",
    "fig.set_ylabel('Count', fontsize=15)\n",
    "fig.set_yticklabels(fig.get_yticks(), fontsize=12)\n",
    "fig.set_xlabel('Borough', fontsize=15)\n",
    "ax.set(ylim=(0, 40))\n",
    "ax.legend(ncol=2, loc=\"upper right\", frameon=True, fontsize=12)\n",
    "plt.xticks(rotation=-35, horizontalalignment='left', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to access HM Land Registry Price Paid Data: https://www.gov.uk/guidance/about-the-price-paid-data\n",
    "\n",
    "Price Paid Data - HM Land Registry: https://www.gov.uk/government/statistical-data-sets/price-paid-data-downloads\n",
    "\n",
    "Average private rental prices per borough:https://data.london.gov.uk/dataset/average-private-rents-borough\n",
    "\n",
    "Borough property and rental prices - Foxtons: https://www.foxtons.co.uk/living-in/bermondsey\n",
    "\n",
    "List of London boroughs : https://en.wikipedia.org/wiki/List_of_London_boroughs\n",
    "\n",
    "London Borough GeoJSON: https://joshuaboyd1.carto.com/tables/london_boroughs_proper/public"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/36631163/what-are-the-pros-and-cons-between-get-dummies-pandas-and-onehotencoder-sciki/38650886#38650886\n",
    "\n",
    "https://stats.stackexchange.com/questions/187595/clustering-with-categorical-and-numeric-data\n",
    "\n",
    "https://www.ritchieng.com/machinelearning-one-hot-encoding/\n",
    "\n",
    "https://towardsdatascience.com/clustering-burger-venues-in-s%C3%A3o-paulo-f4bfc0a031cd\n",
    "\n",
    "https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a\n",
    "\n",
    "k mode\n",
    "https://www.kaggle.com/ashydv/bank-customer-clustering-k-modes-clustering\n",
    "\n",
    "https://stackoverflow.com/questions/42639824/python-k-modes-explanation\n",
    "\n",
    "https://medium.com/@davidmasse8/unsupervised-learning-for-categorical-data-dd7e497033ae"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_geodata_vis",
   "language": "python",
   "name": "python3_geodata_vis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
